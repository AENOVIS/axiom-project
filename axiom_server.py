import os
import uvicorn
import requests
import json
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List

# --- Configuration ---
OLLAMA_API_URL = "http://localhost:11434/api/chat"
TEXT_MODEL_NAME = "llama3:8b"
VISION_MODEL_NAME = "llava" # Assurez-vous d'avoir fait 'ollama pull llava'

# --- Initialisation de l'application FastAPI ---
app = FastAPI()

# --- Middleware CORS ---
# Permet √† notre page web (servie sur un domaine) de communiquer avec notre API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Accepte les requ√™tes de n'importe o√π (simple pour le dev)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Structure des donn√©es pour les requ√™tes ---
class ChatRequest(BaseModel):
    text: str
    image: Optional[str] = None # L'image sera une cha√Æne de caract√®res en Base64
    history: List[dict]

# --- Servir les fichiers statiques (CSS) ---
# Le dossier 'static' contiendra notre fichier style.css
# Cr√©ez un dossier nomm√© 'static' et mettez y style.css
if not os.path.exists('static'):
    os.makedirs('static')
app.mount("/static", StaticFiles(directory="static"), name="static")


# --- Route Principale pour l'interface web ---
@app.get("/", response_class=HTMLResponse)
async def get_root():
    """Sert notre fichier d'interface index.html."""
    if os.path.exists("index.html"):
        return FileResponse("index.html")
    return HTMLResponse("<h1>Fichier index.html non trouv√©.</h1>")

# --- Route API pour le chat ---
@app.post("/api/chat")
async def chat_with_axiom(request: ChatRequest):
    """
    Re√ßoit le message de l'utilisateur, interagit avec Ollama et renvoie la r√©ponse.
    """
    conversation_history = request.history
    current_model = TEXT_MODEL_NAME
    
    # Pr√©pare le nouveau message de l'utilisateur
    user_message = {"role": "user", "content": request.text}

    # Si une image est envoy√©e, on utilise le mod√®le de vision (LLaVA)
    if request.image:
        print("üñºÔ∏è Image d√©tect√©e, utilisation du mod√®le de vision LLaVA.")
        current_model = VISION_MODEL_NAME
        # Le format pour llava avec image est un peu diff√©rent
        user_message["images"] = [request.image]
    
    conversation_history.append(user_message)

    # Pr√©pare le payload pour Ollama
    payload = {
        "model": current_model,
        "messages": conversation_history,
        "stream": False
    }

    print(f"ü§ñ Envoi de la requ√™te √† Ollama avec le mod√®le {current_model}...")

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=180)
        response.raise_for_status() # L√®ve une exception si erreur HTTP
        
        response_json = response.json()
        assistant_message = response_json.get("message", {})
        
        # Ajoute la r√©ponse de l'assistant √† l'historique pour le prochain tour
        conversation_history.append(assistant_message)
        
        return {"response": assistant_message.get("content", ""), "history": conversation_history}

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur de communication avec Ollama: {e}")
        return {"response": f"D√©sol√©, une erreur est survenue en contactant le moteur Axiom. D√©tails: {e}", "history": conversation_history}

# --- Point d'entr√©e pour lancer le serveur ---
if __name__ == "__main__":
    # Uvicorn va lancer notre application FastAPI.
    # host="0.0.0.0" le rend accessible depuis l'ext√©rieur de la machine (essentiel sur le cloud)
    uvicorn.run(app, host="0.0.0.0", port=8000)
